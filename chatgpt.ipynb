{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHATGPT JOKEBOT\n",
    "\n",
    "Building a joke bot using the ChatGPT API and Gradle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"YOUR_API_KEY\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize chatGPT\n",
    "* Assign the user role and ask a question for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"The capital of Norway is Oslo.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1680030979,\n",
      "  \"id\": \"chatcmpl-6z95nRP9Hid6JStnXeAZLUyvEbi0q\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 7,\n",
      "    \"prompt_tokens\": 15,\n",
      "    \"total_tokens\": 22\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = [{\"role\":\"user\", \"content\": \"What is the capital of Norway?\"}]\n",
    ")\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extrect the answer to the question, it should be under the choices list as message, role should be assistant and content would be the answer.\n",
    "* Print it to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Norway is Oslo.\n"
     ]
    }
   ],
   "source": [
    "reply_content = completion.choices[0].message.content\n",
    "#reply_content2 = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "print(reply_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need to keep track of the history of questions asked\n",
    "* We need to ask for user input, this will be important for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<  What is the capital of Nigeria?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input is What is the capital of Nigeria?\n"
     ]
    }
   ],
   "source": [
    "message_history = []\n",
    "\n",
    "user_input = input(\">: \")\n",
    "\n",
    "print(f\"User input is {user_input}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Append the message history and change content to user input instead of writing out the question every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history.append({\"role\":\"user\", \"content\":user_input})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rewrite the completion variable message_history as input\n",
    "* Do the same with reply_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = message_history,\n",
    ")\n",
    "\n",
    "reply_content = completion.choices[0].message.content\n",
    "print(reply_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you want to force chat GPT to give a speciefic answer, you can supply one and append it to message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#message_history.append({\"role\":\"assistant\", \"content\":user_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<  What is the capital of Nigeria?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input is What is the capital of Nigeria?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\">: \")\n",
    "\n",
    "print(f\"User input is {user_input}\\n\")\n",
    "message_history.append({\"role\":\"user\", \"content\":user_input})\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = message_history,\n",
    ")\n",
    "\n",
    "reply_content = completion.choices[0].message.content\n",
    "print(reply_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is essentially the full program, recurring until you type \"exit\" to quit the program.\n",
    "* Use a while loop to keep the program running until you type \"exit\" to quit the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user input is: What is the capital of Nigeria?\n",
      "\n",
      "Abuja.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_history = []\n",
    "\n",
    "def chat(inp, role = \"user\"):\n",
    "    message_history.append({\"role\": role, \"content\": inp})\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = message_history,)\n",
    "\n",
    "    reply_content = completion.choices[0].message.content\n",
    "    print(reply_content)\n",
    "\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": reply_content})\n",
    "    return reply_content\n",
    "\n",
    "while True:\n",
    "#for i in range():\n",
    "    user_input = input(\">: \")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    print(f\"The user input is: {user_input}\\n\")\n",
    "    chat(user_input)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Time to make this more presentable with a GUI\n",
    "* Import gradio and openai modules\n",
    "* Supply your api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize the message history and instruct ChatGPT to be a joke bot\n",
    "* Force it by setting the initial answer for the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = [{\"role\": \"user\", \"content\": f\"You are a joke bot. I will specify the subject matter in my messages, and you will reply with a joke that includes the subjects I mention in my messages. Reply only with jokes to further input. If you understand, say OK.\"},\n",
    "                   {\"role\": \"assistant\", \"content\": f\"OK\"}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a function to compress everything we've done so far to prepare the bot for deployment on gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input):\n",
    "    # tokenize the new input sentence\n",
    "    message_history.append({\"role\": \"user\", \"content\": f\"{input}\"})\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=message_history\n",
    "    )\n",
    "    #Just the reply text\n",
    "    reply_content = completion.choices[0].message.content#.replace('```python', '<pre>').replace('```', '</pre>')\n",
    "    \n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{reply_content}\"}) \n",
    "    \n",
    "    # get pairs of msg[\"content\"] from message history, skipping the pre-prompt:              here.\n",
    "    response = [(message_history[i][\"content\"], message_history[i+1][\"content\"]) for i in range(2, len(message_history)-1, 2)]  # convert to tuples of list\n",
    "    return response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Launch Gradio and assign our function to the text box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a new Blocks app and assigns it to the variable demo.\n",
    "with gr.Blocks() as demo: \n",
    "\n",
    "    # creates a new Chatbot instance and assigns it to the variable chatbot.\n",
    "    chatbot = gr.Chatbot() \n",
    "\n",
    "    # creates a new Row component, which is a container for other components.\n",
    "    with gr.Row(): \n",
    "        '''creates a new Textbox component, which is used to collect user input. \n",
    "        The show_label parameter is set to False to hide the label, \n",
    "        and the placeholder parameter is set'''\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "    '''\n",
    "    sets the submit action of the Textbox to the predict function, \n",
    "    which takes the input from the Textbox, the chatbot instance, \n",
    "    and the state instance as arguments. \n",
    "    This function processes the input and generates a response from the chatbot, \n",
    "    which is displayed in the output area.'''\n",
    "    txt.submit(predict, txt, chatbot) # submit(function, input, output)\n",
    "    '''\n",
    "    sets the submit action of the Textbox to a JavaScript function that returns an empty string. \n",
    "    This line is equivalent to the commented out line above, but uses a different implementation. \n",
    "    The _js parameter is used to pass a JavaScript function to the submit method.'''\n",
    "    txt.submit(None, None, txt, _js=\"() => {''}\") \n",
    "         \n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
